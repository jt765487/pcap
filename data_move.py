#!/usr/bin/env python3
"""
This script runs continuously as a background task, designed to process .pcap files generated by an external application.

Uses an AppContext class for managing configuration, state, and logger.

The script's primary responsibilities include:
1. Monitoring the `source_dir` for new .pcap files.
2. Processing the `csv_file_path` for records, assuming each line has the format:
   `<epoch time>,<full file path>,<SHA256-HASH result>`.
   It uses offset tracking and inode/size checks for efficiency and robustness against replacements/truncations.
   Corresponding files are moved from `source_dir` to `work_dir`.
3. Scanning `source_dir` for any "lost files" (partially written or orphaned files) and moving them to `work_dir`.
4. Attempting to send files from `work_dir` to `remote_host_url` with indefinite retry logic for network errors.
5. Monitoring `csv_dir` for modifications, creations, or deletions on `csv_filename` using a watchdog observer, triggering appropriate processing.
6. Periodically scanning `work_dir` for unsent files in the main loop.
7. Handling SIGTERM and SIGINT for graceful shutdown (e.g., when managed by systemd).

Configuration is loaded from 'config.ini' located in the same directory as the script.
"""

import configparser
import csv
import functools
import io
import logging
import logging.handlers
import os
import shutil
import signal
import sys
import time
from pathlib import Path
from typing import Optional, Tuple, Dict, Any

import requests
from requests.exceptions import Timeout, ConnectionError
from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer

# -----------------------------------------------------------------------------
# APPLICATION CONTEXT CLASS
# -----------------------------------------------------------------------------


class AppContext:
    """Holds application configuration, state, and logger. Logger MUST be configured before we use"""

    def __init__(self, config: Dict[str, Any]):
        self.log = logging.getLogger("pcapUploader")
        self.shutdown_requested = False  # Managed by signal handler

        # Pre-extract common values for easier access
        self.source_dir: Path = config["Directories"]["source_dir"]
        self.work_dir: Path = config["Directories"]["work_dir"]
        self.done_dir: Path = config["Directories"]["done_dir"]
        self.failed_move_dir: Path = config["Directories"]["failed_move_dir"]
        self.dead_letter_dir: Path = config["Directories"]["dead_letter_dir"]
        self.csv_dir: Path = config["Directories"]["csv_dir"]
        self.directories_dict: Dict[str, Path] = config["Directories"]

        self.csv_filename: str = config["Files"]["csv_filename"]
        self.csv_file_path: Path = config["Files"]["csv_file_path"]

        self.remote_host_url: str = config["Network"]["remote_host_url"]
        self.request_timeout: int = config["Network"]["request_timeout"]
        self.verify_ssl: bool = config["Network"]["verify_ssl"]

        self.work_dir_poll_interval: int = config["Timing"]["work_dir_poll_interval"]
        self.initial_backoff: int = config["Timing"]["initial_backoff"]
        self.max_backoff: int = config["Timing"]["max_backoff"]
        self.move_retry_delay: int = config["Timing"]["move_retry_delay"]

        self.move_max_retries: int = config["Retries"]["move_max_retries"]
        self.max_collision_attempts: int = config["Retries"]["max_collision_attempts"]


# -----------------------------------------------------------------------------
# CONFIGURATION FILE HANDLING
# -----------------------------------------------------------------------------
CONFIG_FILE_NAME = "config.ini"
script_dir = Path(__file__).parent.resolve()
DEFAULT_CONFIG_PATH = script_dir / CONFIG_FILE_NAME


def load_configuration(config_path: Path) -> Optional[Dict[str, Any]]:
    """
    Loads, validates, and processes configuration from a specified INI file path.

    Reads the INI file using configparser, expecting sections like [Directories],
    [Files], [Network], [Timing], [Retries], and [Logging]. It performs
    necessary type conversions (e.g., to resolved pathlib.Path objects for
    directories, integers for timings/retries, booleans for flags). It also
    validates certain values (e.g., URL format, positive timeouts) and
    maps the logging level string to the corresponding logging constant.

    Directory paths specified in the config file are converted to absolute,
    resolved Path objects to ensure consistency. The full path to the
    CSV file (`csv_file_path`) is constructed automatically based on the
    `csv_dir` and `csv_filename` values.

    If any critical error occurs during file reading, parsing, type
    conversion, or validation (e.g., file not found, missing required
    options, invalid values), a critical message is printed to stderr,
    and the function returns None, indicating failure to load a valid
    configuration.
    """
    if not config_path.exists():
        print(f"CRITICAL: Config file not found: {config_path}", file=sys.stderr)
        return None
    config = configparser.ConfigParser()
    try:
        config.read(config_path)
    except configparser.Error as e:
        print(f"CRITICAL: Error parsing config {config_path}: {e}", file=sys.stderr)
        return None

    app_config = {}
    try:
        # Directories
        dirs = {
            "source_dir": Path(config.get("Directories", "source_dir")).resolve(),
            "work_dir": Path(config.get("Directories", "work_dir")).resolve(),
            "csv_dir": Path(config.get("Directories", "csv_dir")).resolve(),
            "done_dir": Path(config.get("Directories", "done_dir")).resolve(),
            "failed_move_dir": Path(
                config.get("Directories", "failed_move_dir")
            ).resolve(),
            "dead_letter_dir": Path(
                config.get("Directories", "dead_letter_dir")
            ).resolve(),
        }
        app_config["Directories"] = dirs

        # Files
        csv_filename = config.get("Files", "csv_filename")
        app_config["Files"] = {"csv_filename": csv_filename}
        app_config["Files"]["csv_file_path"] = dirs["csv_dir"] / csv_filename

        # Network
        app_config["Network"] = {
            "remote_host_url": config.get("Network", "remote_host_url"),
            "request_timeout": config.getint("Network", "request_timeout"),
            "verify_ssl": config.getboolean("Network", "verify_ssl"),
        }

        # Timing
        app_config["Timing"] = {
            "work_dir_poll_interval": config.getint("Timing", "work_dir_poll_interval"),
            "initial_backoff": config.getint("Timing", "initial_backoff"),
            "max_backoff": config.getint("Timing", "max_backoff"),
            "move_retry_delay": config.getint("Timing", "move_retry_delay"),
        }

        # Retries
        app_config["Retries"] = {
            "move_max_retries": config.getint("Retries", "move_max_retries"),
            "max_collision_attempts": config.getint(
                "Retries", "max_collision_attempts"
            ),
        }

        # Logging
        log_level_str = config.get("Logging", "log_level", fallback="INFO").upper()
        log_level_map = {
            "DEBUG": logging.DEBUG,
            "INFO": logging.INFO,
            "WARNING": logging.WARNING,
            "ERROR": logging.ERROR,
            "CRITICAL": logging.CRITICAL,
        }
        log_level = log_level_map.get(log_level_str)
        if log_level is None:
            print(
                f"WARNING: Invalid log_level '{log_level_str}'. Using INFO.",
                file=sys.stderr,
            )
            log_level, log_level_str = logging.INFO, "INFO"
        app_config["Logging"] = {
            "log_level": log_level,
            "log_level_name": log_level_str,
        }

        # Validation
        if not app_config["Network"]["remote_host_url"].startswith(
            ("http://", "https://")
        ):
            raise ValueError("remote_host_url must start with http:// or https://")
        if app_config["Network"]["request_timeout"] <= 0:
            raise ValueError("request_timeout must be positive")

    except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e:
        print(f"CRITICAL: Invalid/missing config: {e}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"CRITICAL: Unexpected error loading config: {e}", file=sys.stderr)
        return None

    return app_config


# -----------------------------------------------------------------------------
# LOGGING SETUP
# -----------------------------------------------------------------------------
def setup_logging(
    log_level: int, log_level_name: str
) -> None:  # Return value not needed
    """Sets up logging handlers (Console, Syslog)."""
    log = logging.getLogger("pcapUploader")
    log.setLevel(log_level)
    if log.hasHandlers():
        log.handlers.clear()

    # Console
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    log.addHandler(ch)

    # Syslog
    syslog_path = Path("/dev/log")
    if syslog_path.is_socket():
        try:
            sh = logging.handlers.SysLogHandler(address=str(syslog_path))
            sh.setFormatter(logging.Formatter("%(name)s: %(levelname)s %(message)s"))
            log.addHandler(sh)
            log.info("Syslog handler enabled.")
        except Exception as e:
            log.warning(f"Syslog init failed: {e}. Console only.")
    else:
        log.info("Syslog socket not found. Console only.")

    log.info(f"Logging initialized at level {log_level_name} ({log_level}).")


# -----------------------------------------------------------------------------
# SIGNAL HANDLING (Uses Context, _frame)
# -----------------------------------------------------------------------------
def handle_signal(context: AppContext, signum, _frame):
    """Sets the shutdown flag in the context when receiving SIGINT or SIGTERM."""
    context.log.warning(
        f"Received signal {signal.Signals(signum).name} ({signum}). Requesting shutdown..."
    )

    context.shutdown_requested = True


# -----------------------------------------------------------------------------
# UTILITY FUNCTIONS (Use Context)
# -----------------------------------------------------------------------------
def check_directories_same_filesystem(context: AppContext) -> bool:
    """
    Checks that all configured directories (from context) are on the same file system.
    We move files between these directories, so they must be on the same fs to make
    the move atomic.
    """
    # Use the dictionary stored in context for this check
    directories = context.directories_dict
    device_ids = {}
    reference_dev = None
    first_key = None
    for key, dir_path in directories.items():
        check_path = dir_path if dir_path.exists() else dir_path.parent
        try:
            if not check_path.exists():
                context.log.error(
                    f"Cannot check fs for '{dir_path}' ({key}), check path '{check_path}' missing."
                )
                return False
            st = check_path.stat()
            dev_id = st.st_dev
            device_ids[key] = dev_id
            if reference_dev is None:
                reference_dev, first_key = dev_id, key
            elif dev_id != reference_dev:
                context.log.critical(
                    f"Dir '{dir_path}' ({key},dev {dev_id}) on different fs than '{directories[first_key]}' ({first_key},dev {reference_dev})."
                )
                return False
        except Exception as e:
            context.log.error(
                f"Error getting fs info for '{check_path}' (from '{key}'): {e}"
            )
            return False

    context.log.info("All configured directories appear to be on the same filesystem.")

    return True


def generate_unique_path(
    context: AppContext, target_dir: Path, original_filename: str
) -> Path:
    """Generates a unique file path, accessing context for logger and config."""
    original_path = Path(original_filename)
    dest_file_path = target_dir / original_path.name

    if not dest_file_path.exists():
        return dest_file_path

    base_name, extension = original_path.stem, original_path.suffix
    counter = 0
    max_attempts = context.max_collision_attempts  # From context

    while dest_file_path.exists() and counter < max_attempts:
        counter += 1
        dest_file_path = target_dir / f"{base_name}_{counter}{extension}"

    if counter >= max_attempts:
        msg = f"Could not generate unique name for {original_filename} in {target_dir} after {max_attempts} attempts."
        context.log.error(msg)
        raise Exception(msg)

    return dest_file_path


def move_file_safely(
    context: AppContext,
    src_path: Path,
    target_dir: Path,
    final_filename: Optional[str] = None,
) -> Tuple[bool, Optional[Path]]:
    """Moves src_path to target_dir, ensuring unique name and retrying on errors.

    Generates a unique destination path within target_dir (handling potential
    name collisions) using generate_unique_path. It then attempts to move the
    file using shutil.move. If the move fails due to transient errors, it
    retries based on the context's move_max_retries and move_retry_delay settings.
    Checks for context.shutdown_requested before each attempt and retry sleep.

    Args:
        context: The application context containing config (retries, delay) and state (shutdown_requested).
        src_path: The valid Path object representing the source file to move.
        target_dir: The valid Path object representing the destination directory.
        final_filename: If provided, this name is used for the destination file
                        instead of the source file's name.

    Returns:
        A tuple (success_status, final_destination_path):
        - (True, Path_to_destination) if the move succeeded.
        - (False, None) if the source file was invalid, a unique destination
          name couldn't be generated, the move failed after all retries,
          or the operation was aborted due to a shutdown request.
    """
    if not src_path.is_file():
        context.log.warning(f"Move: Src '{src_path}' not found/not file.")
        return False, None

    effective_filename = final_filename if final_filename else src_path.name
    try:
        dest_path = generate_unique_path(context, target_dir, effective_filename)
    except Exception as e:
        context.log.critical(
            f"Move: Failed generate unique path for '{effective_filename}' in '{target_dir}': {e}. Cannot move '{src_path}'."
        )
        return False, None

    max_retries, retry_delay = (
        context.move_max_retries,
        context.move_retry_delay,
    )
    for attempt in range(max_retries):
        if context.shutdown_requested:
            context.log.warning(f"Move: Shutdown during attempt for {src_path}. Abort.")
            return False, None
        try:
            shutil.move(src_path, dest_path)
            context.log.info(
                f"Moved '{src_path}' to '{dest_path}' (attempt {attempt + 1})."
            )
            return True, dest_path
        except Exception as e:
            context.log.error(
                f"Move attempt {attempt + 1}/{max_retries} for '{src_path}' -> '{dest_path}': {e}"
            )
            if attempt < max_retries - 1:
                if context.shutdown_requested:
                    context.log.warning(
                        f"Move: Shutdown during retry sleep for {src_path}. Abort."
                    )
                    return False, None
                time.sleep(retry_delay)
    context.log.critical(
        f"CRITICAL: Failed move '{src_path}' -> '{dest_path}' after {max_retries} attempts."
    )

    return False, None


def handle_failed_move(
    context: AppContext, original_src_path: Path, intended_dest_dir: Path
):
    """Attempts to move a file that failed its primary move to FAILED_MOVE_DIR (from context)."""
    failed_move_dir = context.failed_move_dir
    context.log.warning(
        f"Attempting move of '{original_src_path}' to failed move dir '{failed_move_dir}' due to persistent failure moving to '{intended_dest_dir}'."
    )
    success, _ = move_file_safely(context, original_src_path, failed_move_dir)
    if not success:
        context.log.critical(
            f"CRITICAL: Also failed move '{original_src_path}' to failed move dir '{failed_move_dir}'. File remains at '{original_src_path}'."
        )


def handle_failed_upload(context: AppContext, file_path: Path):
    """Handles a file that failed upload (non-network/permanent error) by moving it to DEAD_LETTER_DIR (from context)."""
    dead_letter_dir = context.dead_letter_dir
    context.log.warning(
        f"Moving '{file_path}' to dead letter dir '{dead_letter_dir}' due to permanent upload failure."
    )
    success, final_path = move_file_safely(context, file_path, dead_letter_dir)
    if success:
        context.log.info(
            f"Moved file '{file_path.name}' to dead letter directory: {final_path}"
        )
    else:
        context.log.critical(
            f"CRITICAL: Failed move '{file_path}' to dead letter dir '{dead_letter_dir}'. File remains in '{context.work_dir}'."
        )


def extract_full_path_from_line(context: AppContext, line: str) -> Optional[Path]:
    """Extracts the full file path from a CSV-formatted string line.

    The expected CSV file structure for each line is as follows:
    - The first element is an epoch timestamp (e.g., '1678886400').
    - The second element is the full file path (e.g., '/var/data/file.pcap').
    - The third element is a SHA256-HASH result (e.g., 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855').

    Example valid line:
    '1678886400,/var/data/file.pcap,e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
    """
    stripped_line = line.strip()
    if not stripped_line:
        return None
    try:
        f = io.StringIO(stripped_line)
        reader = csv.reader(f, skipinitialspace=True)
        parts = next(reader)
        try:
            next(reader)
            context.log.warning(
                f"CSV line has multiple logical rows; using first. Line: '{stripped_line[:100]}...'"
            )
        except StopIteration:
            pass

        if len(parts) >= 2:
            path_str = parts[1].strip()
            if path_str:
                return Path(path_str)
            else:
                context.log.error(
                    f"CSV path field empty. Line: '{stripped_line[:100]}...'"
                )
                return None
        else:
            context.log.error(
                f"CSV needs >= 2 fields. Line: '{stripped_line[:100]}...', Fields: {parts}"
            )
            return None
    except StopIteration:
        context.log.warning(
            f"Could not parse fields (StopIteration). Line: '{stripped_line[:100]}...'"
        )
        return None
    except csv.Error as e:
        context.log.error(f"CSV format error. Err:{e}. Line:'{stripped_line[:100]}...'")
        return None
    except Exception as e:
        context.log.error(
            f"Unexpected error parsing CSV line. Err:{e}. Line:'{stripped_line[:100]}...'",
            exc_info=True,
        )
        return None


# -----------------------------------------------------------------------------
# CORE LOGIC FUNCTIONS (Use Context, conditional logging)
# -----------------------------------------------------------------------------


def process_csv_updates_from_offset(
    context: AppContext, csv_file_path: Path, start_offset: int
) -> int:
    """Processes new CSV records from offset. Uses context for config, logger, state."""
    current_offset = start_offset
    counts = {
        "moved": 0,
        "parse_err": 0,
        "path_err": 0,
        "not_found": 0,
        "rm_fail": 0,
        "move_fail": 0,
        "read_err": 0,
    }
    context.log.info(
        f"Processing CSV {csv_file_path.name} from offset {current_offset}"
    )
    abs_src_dir_str = str(context.source_dir.resolve())

    try:
        with csv_file_path.open("rb") as csv_file:
            csv_file.seek(current_offset)
            while not context.shutdown_requested:
                try:
                    line_bytes = csv_file.readline()
                    line = line_bytes.decode(errors="replace") if line_bytes else None
                except Exception as read_err:
                    context.log.error(
                        f"Read err CSV {csv_file_path.name} near offset {csv_file.tell()}:{read_err}"
                    )
                    counts["read_err"] += 1
                    current_offset = csv_file.tell()
                    continue
                if not line_bytes:
                    break  # EOF

                full_src_path_obj: Optional[Path] = extract_full_path_from_line(
                    context, line
                )
                if full_src_path_obj:
                    try:
                        resolved_src_path = full_src_path_obj.resolve()
                        is_absolute = full_src_path_obj.is_absolute()
                        src_basename = full_src_path_obj.name
                        if not src_basename:
                            context.log.warning(
                                f"CSV: No basename '{full_src_path_obj}'. Skip."
                            )
                            counts["path_err"] += 1
                        elif not is_absolute:
                            context.log.warning(
                                f"CSV: Path '{full_src_path_obj}' not absolute. Skip."
                            )
                            counts["path_err"] += 1
                        elif not str(resolved_src_path).startswith(
                            abs_src_dir_str + os.sep
                        ):
                            context.log.warning(
                                f"CSV: Path '{resolved_src_path}' outside src '{abs_src_dir_str}'. Orig:'{full_src_path_obj}'. Skip."
                            )
                            counts["path_err"] += 1
                        else:
                            src_file_path = resolved_src_path
                            dest_file_path_in_work = context.work_dir / src_basename
                            if src_file_path.is_file():
                                if dest_file_path_in_work.exists():
                                    context.log.warning(
                                        f"CSV: Target '{src_basename}' exists in '{context.work_dir}'. Removing."
                                    )
                                    try:
                                        dest_file_path_in_work.unlink()
                                    except Exception as e:
                                        context.log.error(
                                            f"CSV: FAILED remove '{dest_file_path_in_work}': {e}. Skip move '{src_file_path}'."
                                        )
                                        counts["rm_fail"] += 1
                                        current_offset = csv_file.tell()
                                        continue
                                move_success, _ = move_file_safely(
                                    context,
                                    src_file_path,
                                    context.work_dir,
                                    src_basename,
                                )
                                if move_success:
                                    counts["moved"] += 1
                                else:
                                    handle_failed_move(
                                        context, src_file_path, context.work_dir
                                    )
                                    counts["move_fail"] += 1
                            else:
                                # Conditional logging for not found
                                log_msg = f"CSV: Source file '{src_file_path}' not found or is not a regular file."
                                original_path_str = str(full_src_path_obj)
                                resolved_path_str = str(src_file_path)
                                if original_path_str != resolved_path_str:
                                    log_msg += f" (Original path from CSV: '{original_path_str}')"
                                context.log.warning(log_msg)
                                counts["not_found"] += 1
                    except OSError as pe:
                        context.log.warning(
                            f"CSV: Path error '{full_src_path_obj}': {pe}. Skip."
                        )
                        counts["path_err"] += 1
                    except Exception as pe:
                        context.log.warning(
                            f"CSV: Unexpected path error '{full_src_path_obj}': {pe}. Skip.",
                            exc_info=True,
                        )
                        counts["path_err"] += 1
                else:
                    counts["parse_err"] += 1
                current_offset = csv_file.tell()
            if context.shutdown_requested:
                context.log.warning(
                    f"CSV processing interrupted by shutdown at offset {current_offset}."
                )
    except FileNotFoundError:
        context.log.error(
            f"CSV file {csv_file_path} not found for processing from {start_offset}."
        )
        return start_offset
    except Exception as e:
        context.log.error(
            f"Unexpected error processing CSV {csv_file_path} from {start_offset}: {e}",
            exc_info=True,
        )
        return current_offset
    context.log.info(
        f"Finished CSV chunk {start_offset}->{current_offset}. Moved:{counts['moved']}, NotFound:{counts['not_found']}, MoveFail:{counts['move_fail']}, ParseErr:{counts['parse_err']}, PathErr:{counts['path_err']}, RmFail:{counts['rm_fail']}, ReadErr:{counts['read_err']}"
    )
    return current_offset


def recover_lost_files(context: AppContext):
    """Scans source directory for lost files and moves them to work dir. Uses context."""
    src_dir, work_dir = context.source_dir, context.work_dir
    if not src_dir.is_dir():
        context.log.warning(f"LostFile: SrcDir {src_dir} not found/dir. Skip.")
        return

    context.log.info(f"Scanning {src_dir} for lost files...")
    counts = {"rec": 0, "err": 0, "rm_fail": 0, "move_fail": 0}

    try:
        for entry in src_dir.iterdir():
            if context.shutdown_requested:
                context.log.warning("Lost file recovery interrupted.")
                break
            if entry.is_file():
                src_file_path, file_name = entry, entry.name
                dest_file_path_in_work = work_dir / file_name
                if dest_file_path_in_work.exists():
                    context.log.warning(
                        f"LostFile: Target '{file_name}' exists in {work_dir}. Removing."
                    )
                    try:
                        dest_file_path_in_work.unlink()
                    except Exception as e:
                        context.log.error(
                            f"LostFile: FAILED remove '{dest_file_path_in_work}': {e}. Skip '{src_file_path}'."
                        )
                        counts["rm_fail"] += 1
                        continue
                move_success, _ = move_file_safely(context, src_file_path, work_dir)
                if move_success:
                    counts["rec"] += 1
                else:
                    handle_failed_move(context, src_file_path, work_dir)
                    counts["move_fail"] += 1
    except Exception as e:
        context.log.error(
            f"Error listing src {src_dir} during lost file recovery: {e}", exc_info=True
        )
        counts["err"] += 1
    context.log.info(
        f"Finished lost file scan. Recovered:{counts['rec']}, MoveFail:{counts['move_fail']}, RmFail:{counts['rm_fail']}, ListErr:{counts['err']}"
    )


def send_file(context: "AppContext", file_path: Path) -> None:
    """Uploads the file to the remote host via HTTP POST with retries and error handling.
    (Docstring remains the same as before)
    """
    if not file_path.is_file():
        context.log.error(f"Send: Path not file/exists at start: {file_path}")
        return

    file_name = file_path.name
    attempt = 1
    backoff = context.initial_backoff
    # REMOVED: response = None # Not needed

    while not context.shutdown_requested:
        try:
            context.log.info(
                f"Attempt {attempt}: Uploading '{file_name}' to {context.remote_host_url}"
            )
            headers = {
                "x-filename": file_name,
                "Content-Type": "application/octet-stream",
            }

            # --- Specific catch for FileNotFoundError around open() ---
            try:
                with file_path.open("rb") as f:
                    # 'response' is created here if successful
                    response = requests.post(
                        context.remote_host_url,
                        data=f,
                        headers=headers,
                        timeout=context.request_timeout,
                        verify=context.verify_ssl,
                    )
            except FileNotFoundError:
                context.log.error(f"File {file_path} disappeared before upload attempt {attempt}. Aborting upload for this file.")
                break # Exit the retry loop for this file
            # --- End specific catch ---

            # --- Process the response (only reached if requests.post succeeded) ---
            if 200 <= response.status_code < 300:
                context.log.info(
                    f"Upload OK for '{file_name}' (status {response.status_code}). Moving to '{context.done_dir}'."
                )
                move_success, final_done_path = move_file_safely(
                    context, file_path, context.done_dir
                )
                if move_success:
                    context.log.info(f"Moved uploaded file to {final_done_path}")
                else:
                    context.log.critical(
                        f"CRITICAL: Upload OK for '{file_path}', but move to '{context.done_dir}' FAILED."
                    )
                    failed_marker = file_name + ".upload_ok_move_failed"
                    try:
                        if file_path.exists():
                            unique_failed_path = generate_unique_path(
                                context, context.work_dir, failed_marker
                            )
                            file_path.rename(unique_failed_path)
                            context.log.error(
                                f"Renamed failed-move file in '{context.work_dir}' to '{unique_failed_path.name}' to prevent reprocessing."
                            )
                        else:
                             context.log.error(
                                f"Source file '{file_path}' disappeared before rename after failed move to done_dir."
                            )
                    except Exception as rename_exc:
                        context.log.critical(
                            f"CRITICAL: Failed rename of '{file_path}' using marker '{failed_marker}' after upload OK/move fail: {rename_exc}. Risk of reprocessing!"
                        )
                break # Success or handled failure-after-success

            elif response.status_code in [429, 500, 502, 503, 504]:
                # Retryable HTTP error
                context.log.warning(
                    f"Attempt {attempt}: Temporary HTTP error {response.status_code} for '{file_name}'. "
                    f"Retry in {backoff}s. Response: {response.text[:200]}"
                )
                if context.shutdown_requested:
                    context.log.warning(f"Shutdown requested during retry wait for '{file_name}'.")
                    break
                time.sleep(backoff)
                attempt += 1
                backoff = min(backoff * 2, context.max_backoff)
                continue # Go to next retry attempt

            else:
                # Permanent HTTP error
                context.log.error(
                    f"Upload failed for '{file_name}'. Permanent HTTP error {response.status_code}. "
                    f"Response: {response.text[:500]}"
                )
                handle_failed_upload(context, file_path)
                break # Stop retrying for this file

        except (Timeout, ConnectionError) as net_err:
            # Network errors are retryable
            context.log.warning(
                f"Attempt {attempt}: Network error uploading '{file_name}': {net_err}. Retry in {backoff}s."
            )
            if context.shutdown_requested:
                context.log.warning(f"Shutdown requested during network retry wait for '{file_name}'.")
                break
            time.sleep(backoff)
            attempt += 1
            backoff = min(backoff * 2, context.max_backoff)
            continue # Go to next retry attempt

        except Exception as e:
            # Catch-all for other unexpected errors
            context.log.error(
                f"Unexpected exception during upload attempt {attempt} for {file_path}: {e}",
                exc_info=True,
            )
            handle_failed_upload(context, file_path)
            break # Stop retrying for this file

    # Final check if loop exited due to shut down
    if context.shutdown_requested:
        context.log.info(f"Upload process for '{file_name}' interrupted by shutdown request.")


def process_work_directory(context: AppContext):
    """Scans work directory and attempts to send .pcap files. Uses context. """
    work_dir = context.work_dir
    if not work_dir.is_dir():
        context.log.warning(f"Work dir {work_dir} not found/dir. Skip.")
        return
    try:
        current_files = [
            entry for entry in work_dir.iterdir() if entry.is_file()
        ]
    except Exception as e:
        context.log.error(f"Error listing work dir {work_dir}: {e}", exc_info=True)
        return
    if not current_files:
        return

    context.log.info(
        f"Scanning work dir {work_dir}. Found {len(current_files)} potential files."
    )
    counts = {"sent": 0, "skip_mark": 0, "skip_ext": 0}
    for file_path in current_files:
        if context.shutdown_requested:
            context.log.warning("Work dir processing interrupted.")
            break
        file_name = file_path.name
        if file_name.endswith(".upload_ok_move_failed"):
            counts["skip_mark"] += 1
            continue
        if not file_name.lower().endswith(".pcap"):
            counts["skip_ext"] += 1
            continue
        if file_path.is_file():
            send_file(context, file_path)
            counts["sent"] += 1

    if any(counts.values()):
        context.log.info(
            f"Finished work dir scan. Sent/Attempted:{counts['sent']}, Skipped(Marked):{counts['skip_mark']}, Skipped(Ext):{counts['skip_ext']}"
        )


# -----------------------------------------------------------------------------
# WATCHDOG EVENT HANDLER (Uses Context)
# -----------------------------------------------------------------------------

class CSVEventHandler(FileSystemEventHandler):
    """Watchdog handler for CSV file. Uses AppContext."""

    def __init__(
        self,
        context: AppContext,
        initial_offset: int,
        initial_inode: Optional[int],
        initial_device: Optional[int],
    ):
        super().__init__()
        self.context = context
        self.csv_file_path: Path = context.csv_file_path
        self._csv_file_path_str = str(self.csv_file_path)
        self.current_offset, self.current_inode, self.current_device = (
            initial_offset,
            initial_inode,
            initial_device,
        )
        self.context.log.info(
            f"Handler init. Watching:'{self.csv_file_path}'. State:Offset={self.current_offset},Inode={self.current_inode},Device={self.current_device}"
        )

    def _reset_state(self, reason=""):
        self.context.log.info(
            f"Resetting CSV state ({reason}). Offset=0, Inode/Device=None."
        )
        self.current_offset, self.current_inode, self.current_device = 0, None, None

    def _process_file(self, event_path_str: str):
        if self.context.shutdown_requested:
            self.context.log.warning("Shutdown req, skip CSV event.")
            return
        event_path = Path(event_path_str)
        try:
            if not event_path.is_file():
                self.context.log.warning(
                    f"CSV {event_path} disappeared/not file pre-proc."
                )
                self._reset_state("File disappeared")
                return
            current_stat = event_path.stat()
            inode, device, size = (
                current_stat.st_ino,
                current_stat.st_dev,
                current_stat.st_size,
            )
            start_offset = -1
            if self.current_inode is None or self.current_device is None:
                self.context.log.info(
                    f"Proc '{event_path.name}' from beginning (reset/initial)."
                )
                start_offset = 0
            elif inode == self.current_inode and device == self.current_device:
                if size < self.current_offset:
                    self.context.log.warning(
                        f"CSV '{event_path.name}' truncated (size {size} < offset {self.current_offset}). Proc from beginning."
                    )
                    start_offset = 0
                elif size == self.current_offset:
                    self.context.log.info(
                        f"CSV '{event_path.name}' modified, no new data (size {size} == offset {self.current_offset}). Skip."
                    )
                else:
                    self.context.log.info(
                        f"CSV '{event_path.name}' appended. Proc from offset {self.current_offset}."
                    )
                    start_offset = self.current_offset
            else:
                self.context.log.info(
                    f"CSV '{event_path.name}' replaced (inode/dev change). Proc from beginning."
                )
                start_offset = 0
            if start_offset != -1:
                new_offset = process_csv_updates_from_offset(
                    self.context, self.csv_file_path, start_offset
                )
                self.current_offset, self.current_inode, self.current_device = (
                    new_offset,
                    inode,
                    device,
                )
                self.context.log.info(
                    f"CSV proc event complete. New state: Offset={self.current_offset}, Inode={self.current_inode}, Device={self.current_device}"
                )
        except FileNotFoundError:
            self.context.log.error(f"CSV {event_path} disappeared mid-proc.")
            self._reset_state("FileNotFound mid-proc")
        except Exception as e:
            self.context.log.error(
                f"Error processing CSV event {event_path}: {e}", exc_info=True
            )
            self._reset_state("Unexpected error")

    def on_created(self, event):
        if event.src_path == self._csv_file_path_str and not event.is_directory:
            self.context.log.info(f"Watchdog: CSV created: {event.src_path}")
            self._reset_state("File created")
            self._process_file(event.src_path)

    def on_modified(self, event):
        if event.src_path == self._csv_file_path_str and not event.is_directory:
            self.context.log.info(f"Watchdog: CSV modified: {event.src_path}")
            self._process_file(event.src_path)

    def on_deleted(self, event):
        if event.src_path == self._csv_file_path_str and not event.is_directory:
            self.context.log.info(f"Watchdog: CSV deleted: {event.src_path}")
            self._reset_state("File deleted")


# -----------------------------------------------------------------------------
# MAIN FUNCTION (Uses Context, refined variable init/usage)
# -----------------------------------------------------------------------------

def main():
    config_path_str = os.environ.get("PCAPUPLOADER_CONFIG", str(DEFAULT_CONFIG_PATH))
    config_path = Path(config_path_str).resolve()
    print(f"Loading configuration from: {config_path}") # No logging yet
    app_config = load_configuration(config_path)
    if app_config is None:
        sys.exit(1)
    try:
        setup_logging(
            app_config["Logging"]["log_level"], app_config["Logging"]["log_level_name"]
        )
    except Exception as e:
        print(f"CRITICAL: Failed init logging: {e}", file=sys.stderr) # No logging yet
        sys.exit(1)

    # Can now use logging for all messages. Can only set up AppContext after logging
    context = AppContext(app_config)

    # Setup signal handlers
    try:
        sig_handler = functools.partial(handle_signal, context)
        signal.signal(signal.SIGTERM, sig_handler)
        signal.signal(signal.SIGINT, sig_handler)
        context.log.info("Registered signal handlers for SIGTERM/SIGINT.")
    except Exception as e:
        context.log.error(f"Error registering signal handlers: {e}")
    context.log.info(
        "Config loaded. SSL verify: {context.verify_ssl}. URL: {context.remote_host_url}"
    )

    context.log.info(f"Watching CSV: {context.csv_file_path}")

    context.log.info("Ensuring directories exist...")
    dir_path_being_created: Optional[Path] = None
    try:
        for (
            dir_name,
            dir_path,
        ) in context.directories_dict.items():
            dir_path_being_created = dir_path
            dir_path.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        log_msg = (
            "CRITICAL: Failed create/access dir"
            + (f" '{dir_path_being_created}'" if dir_path_being_created else "")
            + f": {e}. Exit."
        )
        context.log.critical(log_msg, exc_info=True)
        sys.exit(1)

    context.log.info("Directory check complete.")

    # Check if all directories are on the same filesystem they must be to continue - need atomic moves
    if not check_directories_same_filesystem(context):
        context.log.critical("Dirs must all be on the same filesystems. Exit.")
        sys.exit(1)

    initial_offset, initial_inode, initial_device = 0, None, None
    csv_file_path = context.csv_file_path
    context.log.info("Initial CSV check...")
    try:
        if csv_file_path.is_file():
            context.log.info(f"Found existing CSV: {csv_file_path}. Processing.")
            st = csv_file_path.stat()
            initial_inode, initial_device = st.st_ino, st.st_dev
            initial_offset = process_csv_updates_from_offset(context, csv_file_path, 0)
            context.log.info(
                f"Initial CSV processed. State: Offset={initial_offset}, Inode={initial_inode}, Device={initial_device}"
            )
        else:
            context.log.info(f"CSV {csv_file_path} not found on startup.")
    except Exception as e:
        context.log.error(f"Error initial CSV proc: {e}", exc_info=True)
        initial_offset, initial_inode, initial_device = 0, None, None
        context.log.warning("CSV state potentially inaccurate.")

    context.log.info(f"Scanning '{context.source_dir}' for lost files.")
    recover_lost_files(context)

    context.log.info(f"Setting up watchdog for: {context.csv_dir}")
    event_handler = CSVEventHandler(
        context, initial_offset, initial_inode, initial_device
    )
    observer = Observer()
    try:
        observer.schedule(event_handler, path=str(context.csv_dir), recursive=False)
        observer.start()
        context.log.info("Watchdog started.")
    except Exception as e:
        context.log.critical(
            f"CRITICAL: Failed start watchdog on {context.csv_dir}: {e}. Exit.",
            exc_info=True,
        )
        sys.exit(1)

    poll_interval = context.work_dir_poll_interval
    try:
        context.log.info(
            f"Entering main loop (poll '{context.work_dir}' every {poll_interval}s)..."
        )
        while not context.shutdown_requested:
            process_work_directory(context)
            if not observer.is_alive():
                context.log.critical("CRITICAL: Watchdog thread died. Shutting down.")
                context.shutdown_requested = True
                break
            wait_start = time.monotonic()
            while time.monotonic() < wait_start + poll_interval:
                if context.shutdown_requested:
                    break
                time.sleep(0.5)
    except Exception as e:
        context.log.critical(
            f"CRITICAL: Unhandled main loop exception: {e}", exc_info=True
        )
        context.shutdown_requested = True
    finally:
        context.log.info("Shutdown sequence initiated...")
        if observer.is_alive():
            context.log.info("Stopping watchdog...")
            observer.stop()
            observer.join()
            context.log.info("Watchdog stopped.")
        else:
            context.log.warning("Watchdog already stopped pre-shutdown.")
        context.log.info("pcapUploader service finished.")


# --- Entry Point ---
if __name__ == "__main__":
    if sys.version_info < (3, 6):
        print("Python 3.6+ required.", file=sys.stderr)
        sys.exit(1)
    main()
