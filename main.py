#!/usr/bin/env python3
"""
This script runs continuously as a background task, designed to process .pcap files generated by an external application.

The script's primary responsibilities include:
1. Monitoring the `SRC_DIR` for new .pcap files.
2. Processing the `CSV_FILE_PATH` for records, assuming each line has the format:
   `<epoch time>,<full file path>,<SHA256-HASH result>`.
   It uses offset tracking and inode/size checks for efficiency and robustness against replacements/truncations.
   Corresponding files are moved from `SRC_DIR` to `WORK_DIR`.
3. Scanning `SRC_DIR` for any "lost files" (partially written or orphaned files) and moving them to `WORK_DIR`.
4. Attempting to send files from `WORK_DIR` to `REMOTE_HOST_URL` with indefinite retry logic for network errors.
5. Monitoring `CSV_DIR` for modifications, creations, or deletions on `CSV_FILE_PATH` using a watchdog observer, triggering appropriate processing.
6. Periodically scanning `WORK_DIR` for unsent files in the main loop.

Special Considerations:
* The external application writes to .pcap files in `SRC_DIR`, closes them, and appends a line to the CSV file.
* The external application may overwrite the CSV file on restart.
* The script should attempt to send partially written .pcap files in case of application or system restarts, or CSV file truncation.
* Network errors are handled with indefinite retries.
"""

import csv
import io
import os
import time
import shutil
import logging
import logging.handlers
from typing import Optional, Tuple

import requests
from requests.exceptions import Timeout, ConnectionError
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import sys

# -----------------------------------------------------------------------------
# CONFIGURATION CONSTANTS
# -----------------------------------------------------------------------------
REMOTE_HOST_URL = "http://192.168.0.180:8989/pcap"  # Endpoint for file uploads.
REQUEST_TIMEOUT = 20  # Seconds timeout for upload requests.
VERIFY_SSL = False  # WARNING: Set to True in production with valid HTTPS certificates.

SRC_DIR = "/var/tmp/MOVE/src"
WORK_DIR = "/var/tmp/MOVE/work"
CSV_DIR = "/var/tmp/MOVE/csv"
DONE_DIR = "/var/tmp/MOVE/done"
FAILED_MOVE_DIR = (
    "/var/tmp/MOVE/failed_move"  # Directory for files that failed critical moves
)
DEAD_LETTER_DIR = "/var/tmp/MOVE/dead_letter"  # Directory for files that failed uploads (non-network errors)
CSV_FILE = "SHA256-HASH.csv"  # Each CSV line: <epoch time>,<full file path>,<SHA256-HASH result>
CSV_FILE_PATH = os.path.join(CSV_DIR, CSV_FILE)

WORK_DIR_POLL_INTERVAL = 5  # Seconds between checking WORK_DIR for files to send.
INITIAL_BACKOFF = 1  # Initial backoff in seconds for network retries.
MAX_BACKOFF = 60  # Maximum backoff delay in seconds for network retries.
MOVE_RETRY_DELAY = 1  # Seconds delay between move retries.
MOVE_MAX_RETRIES = 3  # Max attempts for critical file moves.
LOG_LEVEL = logging.INFO  # Logging level.
MAX_COLLISION_ATTEMPTS = (
    1000  # Max attempts for unique naming in DONE/DEAD_LETTER/FAILED_MOVE dirs.
)


# -----------------------------------------------------------------------------
# LOGGING SETUP
# -----------------------------------------------------------------------------
def setup_logging():
    """
    Sets up logging for the service.
    Logs to syslog (/dev/log) and the console.
    """
    logger_instance = logging.getLogger("pcapUploader")
    logger_instance.setLevel(LOG_LEVEL)
    logger_instance.handlers.clear()  # Prevent duplicate handlers if called multiple times

    # Syslog Handler
    try:
        syslog_handler = logging.handlers.SysLogHandler(address="/dev/log")
        formatter = logging.Formatter("%(name)s: %(levelname)s %(message)s")
        syslog_handler.setFormatter(formatter)
        logger_instance.addHandler(syslog_handler)
    except Exception as e:
        print(f"WARNING: Failed to setup syslog handler: {e}", file=sys.stderr)

    # Console Handler
    console_handler = logging.StreamHandler(sys.stdout)  # Use stdout for console
    console_formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    console_handler.setFormatter(console_formatter)
    logger_instance.addHandler(console_handler)

    logger_instance.info(
        f"Logging initialized at level {logging.getLevelName(logger_instance.getEffectiveLevel())}."
    )
    return logger_instance


logger = setup_logging()

# -----------------------------------------------------------------------------
# UTILITY FUNCTIONS
# -----------------------------------------------------------------------------


def generate_unique_path(target_dir: str, original_filename: str) -> str:
    """
    Generates a unique file path within the target directory.
    Appends '_<counter>' before the extension if the original name exists.
    Raises Exception if a unique name cannot be found within MAX_COLLISION_ATTEMPTS.
    """
    dest_file_path = os.path.join(target_dir, original_filename)
    if not os.path.exists(dest_file_path):
        return dest_file_path

    base_name, extension = os.path.splitext(original_filename)
    counter = 0
    while os.path.exists(dest_file_path) and counter < MAX_COLLISION_ATTEMPTS:
        counter += 1
        dest_file_path = os.path.join(target_dir, f"{base_name}_{counter}{extension}")

    if counter >= MAX_COLLISION_ATTEMPTS:
        msg = f"Could not generate a unique name for {original_filename} in {target_dir} after {MAX_COLLISION_ATTEMPTS} attempts."
        logger.error(msg)
        raise Exception(msg)  # Raise exception to signal failure

    return dest_file_path


def move_file_safely(
    src_path: str,
    target_dir: str,
    final_filename: Optional[str] = None,
    max_retries: int = MOVE_MAX_RETRIES,
    retry_delay: int = MOVE_RETRY_DELAY,
) -> Tuple[bool, Optional[str]]:
    """
    Moves a file with retry logic to a target directory, handling name collisions.

    Args:
        src_path: The source file path.
        target_dir: The destination directory - must exist.
        final_filename: The desired filename in the target directory (defaults to basename of src_path).
        max_retries: Maximum number of move attempts.
        retry_delay: Delay between retries in seconds.

    Returns:
        A tuple (bool, Optional[str]):
        - True and the final destination path if the move was successful.
        - False and None if the move failed after all retries.
    """
    if not os.path.exists(src_path):
        logger.warning(f"Attempted to move non-existent file: {src_path}")
        return False, None

    if final_filename is None:
        final_filename = os.path.basename(src_path)

    try:
        dest_path = generate_unique_path(target_dir, final_filename)
    except Exception as e:
        logger.critical(
            f"Failed to generate unique destination path for '{final_filename}' in '{target_dir}': {e}. Cannot move '{src_path}'."
        )
        return False, None  # Cannot proceed if unique name fails

    for attempt in range(max_retries):
        try:
            shutil.move(src_path, dest_path)
            logger.info(f"Moved '{src_path}' to '{dest_path}' (attempt {attempt + 1}).")
            return True, dest_path
        except Exception as e:
            logger.error(
                f"Failed move attempt {attempt + 1}/{max_retries} for '{src_path}' -> '{dest_path}': {e}"
            )
            if attempt < max_retries - 1:
                time.sleep(retry_delay)

    logger.critical(
        f"CRITICAL: Failed to move file '{src_path}' to '{dest_path}' after {max_retries} attempts."
    )
    return False, None  # Failed after all retries


def handle_failed_move(original_src_path: str, intended_dest_dir: str):
    """
    Handles a file that failed to be moved to its intended destination directory
    by attempting to move it to the FAILED_MOVE_DIR.
    Logs critically if this fallback move also fails.
    """
    logger.warning(
        f"Attempting to move '{original_src_path}' to the failed move directory ('{FAILED_MOVE_DIR}') due to persistent failure moving to '{intended_dest_dir}'."
    )
    success, _ = move_file_safely(
        original_src_path, FAILED_MOVE_DIR, max_retries=1
    )  # Only one attempt for fallback
    if not success:
        # If moving to FAILED_MOVE_DIR also fails, the file remains in its original location.
        logger.critical(
            f"CRITICAL: Failed to move '{original_src_path}' to the failed move directory ('{FAILED_MOVE_DIR}'). File remains at its current location."
        )


def handle_failed_upload(file_path: str):
    """
    Handles a file that failed to upload due to non-network errors by moving it
    to the DEAD_LETTER_DIR. Logs critically if the move fails.
    """
    logger.warning(
        f"Moving '{file_path}' to the dead letter directory ('{DEAD_LETTER_DIR}') due to upload failure."
    )
    success, final_path = move_file_safely(file_path, DEAD_LETTER_DIR)
    if success:
        logger.info(f"Moved file '{file_path}' to dead letter directory: {final_path}")
    else:
        # If moving to DEAD_LETTER_DIR fails, the file remains in WORK_DIR
        logger.critical(
            f"CRITICAL: Failed to move file '{file_path}' to the dead letter directory ('{DEAD_LETTER_DIR}'). File remains in '{WORK_DIR}'."
        )


# -----------------------------------------------------------------------------
# CORE PROCESSING FUNCTIONS
# -----------------------------------------------------------------------------
def extract_full_path_from_line(line: str) -> Optional[str]:
    """
    Extracts the full file path (expected as the second field) from a CSV-formatted string line.
    Handles potential quoting and errors gracefully.
    """
    stripped_line = line.strip()
    if not stripped_line:
        return None

    try:
        # Use csv.reader to handle potential quoting correctly
        f = io.StringIO(stripped_line)
        reader = csv.reader(
            f, skipinitialspace=True
        )  # skip initial space handles spaces after comma
        parts = next(reader)  # Get the first (and should be only) record

        # Check if there's more than one record in the line (unlikely but possible with malformed CSV)
        try:
            next(reader)
            logger.warning(
                f"CSV line string contained more than one logical row; only processing the first. Line: '{stripped_line}'"
            )
        except StopIteration:
            pass  # Expected case

        if len(parts) >= 2:
            file_path = parts[1].strip()  # Field 2 (index 1) is the path
            if file_path:
                return file_path
            else:
                # Log error here for empty path field
                logger.error(
                    f"CSV line parsed, but the second field (file path) is empty. Line: '{stripped_line}'"
                )
                return None
        else:
            # Log error here for insufficient fields
            logger.error(
                f"CSV line did not contain at least 2 fields. Line: '{stripped_line}', Parsed Fields: {parts}"
            )
            return None

    except (
        StopIteration
    ):  # Handles empty lines that somehow passed the initial strip check
        logger.warning(
            f"Could not parse any fields from the CSV line (StopIteration). Line: '{stripped_line}'"
        )
        return None
    except csv.Error as e:
        # Log CSV specific error here
        logger.error(
            f"CSV formatting error while parsing line. Error: {e}. Line: '{stripped_line}'"
        )
        return None
    except Exception as e:
        # Log unexpected parsing error here
        logger.error(
            f"Unexpected error parsing CSV line. Error: {e}. Line: '{stripped_line}'",
            exc_info=True,
        )
        return None


def process_csv_updates_from_offset(
    csv_file_path, start_offset, src_dir, work_dir, use_logger
):
    """
    Reads the CSV file starting at the given offset and processes new records.
    For each valid line, attempts to move the referenced file from SRC_DIR to WORK_DIR.
    Returns the new offset after processing.
    """
    current_offset = start_offset  # Start processing from this offset
    processed_count = 0  # Files successfully moved to WORK_DIR
    parse_error_count = 0  # Lines that couldn't be parsed or had invalid format
    path_error_count = 0  # Lines with paths outside SRC_DIR or invalid paths
    not_found_count = 0  # Files mentioned in CSV but not found in SRC_DIR
    remove_failed_count = 0  # Failures removing existing files in WORK_DIR
    move_failed_count = 0  # Files found but failed to move to WORK_DIR (after retries)
    read_error_count = 0  # Errors reading lines from the CSV file

    use_logger.info(f"Processing CSV {csv_file_path} from offset {current_offset}")
    abs_src_dir = os.path.abspath(src_dir)  # Get absolute path for reliable comparison

    try:
        with open(csv_file_path, "rb") as csv_file:
            csv_file.seek(current_offset)
            while True:
                try:
                    line_bytes = csv_file.readline()
                    if not line_bytes:
                        break  # End of file
                    line = line_bytes.decode(
                        errors="replace"
                    )  # Decode the line after reading
                except Exception as read_err:
                    use_logger.error(
                        f"Error reading line from CSV at offset approx {csv_file.tell()}: {read_err}"
                    )
                    read_error_count += 1
                    current_offset = csv_file.tell()
                    continue  # Try next line

                # Process the successfully read line
                full_src_path = extract_full_path_from_line(line)
                if full_src_path:
                    # --- Path Validation ---
                    if not os.path.abspath(full_src_path).startswith(
                        abs_src_dir + os.sep
                    ):
                        use_logger.warning(
                            f"CSV Processing: Path '{full_src_path}' is outside the configured SRC_DIR '{abs_src_dir}'. Skipping."
                        )
                        path_error_count += 1
                    elif not os.path.isabs(full_src_path):
                        use_logger.warning(
                            f"CSV Processing: Path '{full_src_path}' from CSV is not absolute. Skipping."
                        )
                        path_error_count += 1
                    elif not os.path.basename(full_src_path):
                        use_logger.warning(
                            f"Could not extract basename from CSV path: '{full_src_path}'. Skipping."
                        )
                        path_error_count += 1
                    else:
                        src_file_path = full_src_path
                        dest_file_path_in_work = os.path.join(
                            work_dir, os.path.basename(full_src_path)
                        )

                        if os.path.exists(src_file_path) and os.path.isfile(
                            src_file_path
                        ):
                            # Handle existing file in WORK_DIR - remove it first
                            if os.path.exists(dest_file_path_in_work):
                                use_logger.warning(
                                    f"CSV Processing: Target file '{os.path.basename(full_src_path)}' already exists in '{work_dir}'. Removing before move."
                                )
                                try:
                                    os.remove(dest_file_path_in_work)
                                    use_logger.info(
                                        f"CSV Processing: Successfully removed existing file '{dest_file_path_in_work}'."
                                    )
                                except Exception as e:
                                    use_logger.error(
                                        f"CSV Processing: FAILED to remove existing file '{dest_file_path_in_work}': {e}. Skipping move for '{src_file_path}'."
                                    )
                                    remove_failed_count += 1
                                    current_offset = csv_file.tell()
                                    continue

                            # Attempt to move the file
                            move_success, _ = move_file_safely(
                                src_file_path, work_dir, os.path.basename(full_src_path)
                            )
                            if move_success:
                                processed_count += 1
                            else:
                                handle_failed_move(src_file_path, work_dir)
                                move_failed_count += 1
                        else:
                            use_logger.warning(
                                f"CSV Processing: Source file '{src_file_path}' (from CSV) not found or is not a regular file."
                            )
                            not_found_count += 1
                else:
                    use_logger.warning(
                        f"CSV Processing: Skipped line due to parsing error near offset {current_offset}"
                    )
                    parse_error_count += 1

                # Update the current offset to the end of the processed line
                current_offset = csv_file.tell()

    except FileNotFoundError:
        use_logger.error(
            f"CSV file {csv_file_path} not found during processing update from offset {start_offset}."
        )
        return start_offset
    except Exception as e:
        use_logger.error(
            f"Unexpected error processing CSV file {csv_file_path} from offset {start_offset}: {e}",
            exc_info=True,
        )
        return current_offset

    use_logger.info(
        f"Finished processing CSV chunk from offset {start_offset}. New offset: {current_offset}. "
        f"Moved: {processed_count}, Not Found: {not_found_count}, Move Failed: {move_failed_count}, "
        f"Parse Errors: {parse_error_count}, Path Errors: {path_error_count}, "
        f"Remove Failed: {remove_failed_count}, Read Errors: {read_error_count}"
    )
    return current_offset


def recover_lost_files(src_dir, work_dir, use_logger):
    """
    Scans SRC_DIR for any files not already processed (lost files) and moves them to WORK_DIR.
    Handles existing files in WORK_DIR by removing them first.
    Logs each file that is recovered.
    """
    if not os.path.exists(src_dir):
        use_logger.warning(
            f"Source directory {src_dir} does not exist during lost file recovery."
        )
        return
    use_logger.info(f"Scanning {src_dir} for lost files...")
    recovered_count = 0
    error_count = 0  # General errors during listing/processing
    remove_failed_count = 0
    move_failed_count = 0
    try:
        for file_name in os.listdir(src_dir):
            src_file_path = os.path.join(src_dir, file_name)
            if os.path.isfile(src_file_path):
                dest_file_path_in_work = os.path.join(work_dir, file_name)

                # Handle existing file in WORK_DIR - remove it first
                if os.path.exists(dest_file_path_in_work):
                    use_logger.warning(
                        f"Lost File Recovery: File '{file_name}' already exists in {work_dir}. Removing before move."
                    )
                    try:
                        os.remove(dest_file_path_in_work)
                        use_logger.info(
                            f"Lost File Recovery: Successfully removed existing file '{dest_file_path_in_work}'."
                        )  # Log success
                    except Exception as e:
                        use_logger.error(
                            f"Lost File Recovery: FAILED to remove existing file '{dest_file_path_in_work}': {e}. Skipping recovery for '{src_file_path}'."
                        )
                        remove_failed_count += 1
                        continue  # Skip recovery if removal fails

                # Attempt to move the file
                move_success, _ = move_file_safely(src_file_path, work_dir, file_name)
                if move_success:
                    # Log success (move_file_safely logs specifics)
                    recovered_count += 1
                else:
                    # Move failed after retries, attempt to move original source to FAILED_MOVE_DIR
                    handle_failed_move(src_file_path, work_dir)
                    move_failed_count += 1

    except Exception as e:
        use_logger.error(
            f"Error listing source directory {src_dir} during lost file recovery: {e}",
            exc_info=True,
        )
        error_count += 1
    use_logger.info(
        f"Finished scanning for lost files. Recovered: {recovered_count}, Move Failed: {move_failed_count}, Remove Failed: {remove_failed_count}, Other Errors: {error_count}"
    )


def send_file(file_path: str) -> None:
    """
    Uploads a file to REMOTE_HOST_URL with network error retries.
    On successful upload (HTTP 2xx), moves the file to DONE_DIR.
    If the final move to DONE_DIR fails, renames the file in WORK_DIR.
    If upload fails due to errors deemed permanent, moves the file to DEAD_LETTER_DIR.

    Transient errors (network exceptions and temporary HTTP error codes)
    are retried indefinitely using exponential backoff.
    """
    if not os.path.isfile(file_path):
        logger.error(f"Send File: File not found or is not a file: {file_path}")
        return

    file_name = os.path.basename(file_path)
    attempt = 1
    backoff = INITIAL_BACKOFF

    while True:
        try:
            logger.info(f"Attempt {attempt}: Uploading file: {file_path} as '{file_name}'")
            headers = {
                'x-filename': file_name,
                'Content-Type': 'application/octet-stream'
            }
            # Stream the file upload
            with open(file_path, 'rb') as f:
                response = requests.post(
                    REMOTE_HOST_URL,
                    data=f,
                    headers=headers,
                    timeout=REQUEST_TIMEOUT,
                    verify=VERIFY_SSL
                )

            # Check the HTTP status code
            if 200 <= response.status_code < 300:
                logger.info(
                    f"Upload succeeded for {file_path} (Status Code: {response.status_code}). Attempting move to ('{DONE_DIR}').")
                move_success, final_done_path = move_file_safely(file_path, DONE_DIR, file_name)
                if move_success:
                    logger.info(f"Moved successfully uploaded file to {final_done_path}")
                else:
                    logger.critical(
                        f"Upload succeeded for '{file_path}', but move to ('{DONE_DIR}') failed after retries.")
                    failed_path_in_work = file_path + ".upload_ok_move_failed"
                    try:
                        # Attempt to rename within WORK_DIR to prevent duplicate uploads
                        unique_failed_path = generate_unique_path(WORK_DIR, os.path.basename(failed_path_in_work))
                        os.rename(file_path, unique_failed_path)
                        logger.error(
                            f"Renamed file in ('{WORK_DIR}') to '{unique_failed_path}' to prevent duplicate uploads.")
                    except Exception as rename_exc:
                        logger.critical(
                            f"CRITICAL: Failed to rename '{file_path}' to '{failed_path_in_work}' after successful upload and failed move: {rename_exc}. File remains in ('{WORK_DIR}').")
                break  # Exit loop after handling success

            # Transient HTTP errors: retry for rate limiting or server-side temporary issues.
            elif response.status_code in [429, 500, 502, 503, 504]:
                logger.warning(
                    f"Attempt {attempt}: Received temporary HTTP error {response.status_code} for {file_path}. Retrying in {backoff} seconds. Response: {response.text[:500]}"
                )
                time.sleep(backoff)
                attempt += 1
                backoff = min(backoff * 2, MAX_BACKOFF)
                continue

            # For all other HTTP errors, consider them non-recoverable.
            else:
                logger.error(
                    f"Upload failed for {file_path}. Permanent HTTP error {response.status_code} received. Response: {response.text[:500]}"
                )
                handle_failed_upload(file_path)  # Moves file to DEAD_LETTER_DIR
                break

        except (Timeout, ConnectionError) as net_err:
            logger.warning(
                f"Attempt {attempt}: Network error during upload of {file_path}: {net_err}. Retrying in {backoff} seconds."
            )
            time.sleep(backoff)
            attempt += 1
            backoff = min(backoff * 2, MAX_BACKOFF)
            continue
        except Exception as e:
            logger.error(f"Unexpected exception during upload attempt for {file_path}: {e}", exc_info=True)
            handle_failed_upload(file_path)
            break

def process_work_directory(work_dir, logger):
    """
    Scans WORK_DIR for files and attempts to send each one using send_file().
    Only files ending in '.pcap' are considered. Files ending with ".upload_ok_move_failed" are skipped.
    """
    try:
        with os.scandir(work_dir) as entries:
            current_files = [entry.name for entry in entries if entry.is_file()]
    except FileNotFoundError:
        logger.warning(f"Work directory {work_dir} not found during scan. Skipping.")
        return
    except Exception as e:
        logger.error(f"Error listing work directory {work_dir}: {e}", exc_info=True)
        return

    if not current_files:
        return

    logger.info(f"Scanning work directory {work_dir}. Found {len(current_files)} files.")
    processed_count = 0
    for file_name in current_files:
        # Skip files explicitly marked as failed
        if file_name.endswith(".upload_ok_move_failed"):
            logger.debug(f"Skipping file marked as upload_ok_move_failed: {file_name}")
            continue

        # Only process files with a .pcap extension
        if not file_name.endswith(".pcap"):
            logger.info(f"Skipping file '{file_name}' because it is not a .pcap file.")
            continue

        file_path = os.path.join(work_dir, file_name)
        if os.path.isfile(file_path):
            send_file(file_path)
            processed_count += 1
        else:
            logger.warning(f"Item '{file_name}' in work directory list is no longer a file. Skipping.")

    if processed_count > 0:
        logger.info(f"Finished processing {processed_count} files found in {work_dir}.")

class CSVEventHandler(FileSystemEventHandler):
    """
    Watchdog event handler for monitoring the CSV file using offset and stat checks.
    Handles modifications, creations, and deletions, updating its state accordingly.
    """

    def __init__(
        self,
        csv_file_path,
        initial_offset,
        initial_inode,
        initial_device,
        src_dir,
        work_dir,
        use_logger,
    ):
        super().__init__()  # Initialize base class
        self.csv_file_path = csv_file_path
        self.src_dir = src_dir
        self.work_dir = work_dir
        self.logger = use_logger
        self.current_offset = initial_offset
        self.current_inode = initial_inode
        self.current_device = initial_device
        self.logger.info(
            f"Handler initialized. Watching: '{self.csv_file_path}'. "
            f"Offset: {self.current_offset}, Inode: {self.current_inode}, Device: {self.current_device}"
        )

    def _reset_state(self, reason=""):
        """Resets the CSV processing state (offset, inode, device)."""
        self.logger.info(
            f"Resetting CSV processing state ({reason}). Offset=0, Inode=None, Device=None."
        )
        self.current_offset = 0
        self.current_inode = None
        self.current_device = None

    def _process_file(self, event_path):
        """Processes the CSV file based on current state and file stats."""
        try:
            # Check existence immediately before stating
            if not os.path.exists(event_path):
                self.logger.warning(
                    f"CSV file {event_path} disappeared before processing could start."
                )
                self._reset_state(reason="File disappeared before processing")
                return

            current_stat = os.stat(event_path)
            inode = current_stat.st_ino
            device = current_stat.st_dev
            size = current_stat.st_size

            start_offset = -1  # Sentinel value: -1 means do not process

            # --- Determine starting offset based on state ---
            if self.current_inode is None or self.current_device is None:
                # State was reset (e.g., file created, deleted, or initial state)
                self.logger.info(
                    f"Processing '{os.path.basename(event_path)}' from beginning (state was reset or initial)."
                )
                start_offset = 0
            elif inode == self.current_inode and device == self.current_device:
                # Same file as before (inode/device match)
                if size < self.current_offset:
                    self.logger.warning(
                        f"CSV file '{os.path.basename(event_path)}' truncated (size {size} < offset {self.current_offset}). Processing from beginning."
                    )
                    start_offset = 0
                elif size == self.current_offset:
                    # File modified, but size hasn't increased beyond the last read offset.
                    # This could be a timestamp update without new data, or data written *before* the offset.
                    self.logger.info(
                        f"CSV file '{os.path.basename(event_path)}' modified, but no new data detected (size {size} equals offset {self.current_offset}). No processing needed."
                    )
                    # start_offset remains -1
                else:  # size > self.current_offset
                    # File appended
                    self.logger.info(
                        f"CSV file '{os.path.basename(event_path)}' appended. Processing from offset {self.current_offset}."
                    )
                    start_offset = self.current_offset
            else:
                # Different file (inode or device changed) - likely replaced/overwritten
                self.logger.info(
                    f"CSV file '{os.path.basename(event_path)}' replaced (inode/device changed). Processing from beginning."
                )
                start_offset = 0

            # --- Process if start_offset is valid ---
            if start_offset != -1:
                new_offset = process_csv_updates_from_offset(
                    self.csv_file_path,
                    start_offset,
                    self.src_dir,
                    self.work_dir,
                    self.logger,
                )
                # Update state ONLY if processing occurred and was based on the current file identity
                # If the file was truncated or replaced (start_offset=0), we update inode/device.
                # If it was appended (start_offset=self.current_offset), we also update.
                self.current_offset = new_offset
                self.current_inode = inode
                self.current_device = device
                self.logger.info(
                    f"CSV processing complete for this event. New state: Offset={self.current_offset}, Inode={self.current_inode}, Device={self.current_device}"
                )

        except FileNotFoundError:
            # File existed at the start of _process_file but disappeared before stat/processing
            self.logger.error(
                f"CSV file {event_path} not found during processing attempt (disappeared mid-process)."
            )
            self._reset_state(reason="FileNotFoundError during processing")
        except Exception as e:
            self.logger.error(
                f"Error processing CSV event for {event_path}: {e}", exc_info=True
            )
            # Reset the state to start processing from the beginning, ensuring no data is missed.
            self._reset_state(reason="Unexpected error during CSV event processing")

    def on_created(self, event):
        """Called when a file or directory is created."""
        if event.src_path == self.csv_file_path and not event.is_directory:
            self.logger.info(f"Watchdog: CSV file created: {event.src_path}")
            # Reset state because it's a new file, then process from beginning
            self._reset_state(reason="File created")
            self._process_file(event.src_path)

    def on_modified(self, event):
        """Called when a file or directory is modified."""
        if event.src_path == self.csv_file_path and not event.is_directory:
            self.logger.info(f"Watchdog: CSV file modified: {event.src_path}")
            # Process based on current state (handles append, truncate, replace)
            self._process_file(event.src_path)

    def on_deleted(self, event):
        """Called when a file or directory is deleted."""
        if event.src_path == self.csv_file_path and not event.is_directory:
            self.logger.info(f"Watchdog: CSV file deleted: {event.src_path}")
            # Reset state as the file we were tracking is gone
            self._reset_state(reason="File deleted")


# -----------------------------------------------------------------------------
# MAIN FUNCTION
# -----------------------------------------------------------------------------
def main():
    logger.info("Starting pcapUploader service.")

    # List of directories the script interacts with
    all_dirs = [SRC_DIR, WORK_DIR, CSV_DIR, DONE_DIR, DEAD_LETTER_DIR, FAILED_MOVE_DIR]

    # --- Create Directories ---
    # Ensure all necessary directories exist on startup.
    logger.info("Ensuring all operational directories exist...")
    for dir_path in all_dirs:
        try:
            os.makedirs(dir_path, exist_ok=True)
            logger.info(f"Directory verified/created: {dir_path}")
        except Exception as e:
            logger.critical(
                f"CRITICAL: Failed to create or access directory {dir_path}: {e}. Exiting."
            )
            sys.exit(1)
    logger.info("Directory check complete.")

    # --- Initial CSV State Determination & Processing ---
    initial_offset = 0
    initial_inode = None
    initial_device = None

    logger.info(
        "Performing initial CSV file state check and processing any existing content."
    )
    try:
        if os.path.exists(CSV_FILE_PATH) and os.path.isfile(CSV_FILE_PATH):
            logger.info(
                f"Found existing CSV file: {CSV_FILE_PATH}. Processing from beginning."
            )
            initial_stat = os.stat(CSV_FILE_PATH)
            initial_inode = initial_stat.st_ino
            initial_device = initial_stat.st_dev
            # Process the entire existing CSV file on startup
            initial_offset = process_csv_updates_from_offset(
                CSV_FILE_PATH, 0, SRC_DIR, WORK_DIR, logger
            )
            logger.info(
                f"Initial CSV processing complete. Final offset: {initial_offset}, Inode: {initial_inode}, Device: {initial_device}"
            )
        else:
            logger.info(
                f"CSV file {CSV_FILE_PATH} not found or not a file on startup. Watchdog will handle creation."
            )
    except Exception as e:
        logger.error(f"Error during initial CSV processing: {e}", exc_info=True)
        # Reset state to defaults in case of error during initial check
        initial_offset = 0
        initial_inode = None
        initial_device = None
        logger.warning("CSV state reset due to error during initial processing.")

    # --- Lost File Recovery ---
    # Run this *after* initial CSV processing to catch files in SRC_DIR
    # that might not have corresponding entries in the (potentially incomplete) CSV.
    logger.info(
        f"Scanning '{SRC_DIR}' for any remaining 'lost' files."
    )
    recover_lost_files(SRC_DIR, WORK_DIR, logger)

    # --- Setup Watchdog ---
    logger.info(f"Setting up watchdog observer to monitor directory: {CSV_DIR}")
    event_handler = CSVEventHandler(
        csv_file_path=CSV_FILE_PATH,
        initial_offset=initial_offset,
        initial_inode=initial_inode,
        initial_device=initial_device,
        src_dir=SRC_DIR,
        work_dir=WORK_DIR,
        use_logger=logger,
    )
    observer = Observer()
    try:
        # Watch the directory containing the CSV file
        observer.schedule(event_handler, path=CSV_DIR, recursive=False)
        observer.start()
        logger.info(f"Started watchdog observer on directory: {CSV_DIR}")
    except Exception as e:
        logger.critical(
            f"CRITICAL: Failed to start watchdog observer on {CSV_DIR}: {e}. Exiting."
        )
        observer.stop()  # Attempt to stop if started partially
        observer.join()
        sys.exit(1)

    # --- Main Loop ---
    try:
        logger.info(
            f"Entering main processing loop (polling work directory '{WORK_DIR}' and responding to watchdog events)."
        )
        while True:
            # Periodically process any files that have landed in the WORK_DIR
            process_work_directory(WORK_DIR, logger)

            # Check if the observer thread is still alive
            if not observer.is_alive():
                logger.critical(
                    "CRITICAL: Watchdog observer thread is no longer alive. Shutting down."
                )
                break  # Exit main loop

            # Sleep for the polling interval
            time.sleep(WORK_DIR_POLL_INTERVAL)

    except KeyboardInterrupt:
        logger.info(
            "Received KeyboardInterrupt. Stopping observer and shutting down gracefully."
        )
    except Exception as e:
        # Catch unexpected errors in the main loop itself
        logger.critical(
            f"CRITICAL: Unhandled exception in main loop: {e}", exc_info=True
        )
    finally:
        # --- Shutdown ---
        logger.info("Initiating shutdown sequence...")
        if observer.is_alive():
            logger.info("Stopping watchdog observer...")
            observer.stop()
            observer.join()  # Wait for the observer thread to finish
            logger.info("Observer stopped.")
        else:
            logger.warning(
                "Watchdog observer was already stopped before final shutdown."
            )
        logger.info("pcapUploader service finished.")


if __name__ == "__main__":
    # Ensure the script is run with Python 3
    if sys.version_info[0] < 3:
        print("This script requires Python 3 or later.", file=sys.stderr)
        sys.exit(1)

    main()
